{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":567,"status":"ok","timestamp":1728382071933,"user":{"displayName":"VƒÉn Hi·∫øu L√Ω","userId":"02618301898146327018"},"user_tz":-420},"id":"2w_CS9qClE7S"},"outputs":[],"source":["# !pip install transformers datasets"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20207,"status":"ok","timestamp":1728382092137,"user":{"displayName":"VƒÉn Hi·∫øu L√Ω","userId":"02618301898146327018"},"user_tz":-420},"id":"W-Q1B4pDljhM","outputId":"fc70b32e-cb28-4cd9-931c-14e6e6c2907d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True) # th√™m force_remount=True ƒë·ªÉ ƒë·∫£m b·∫£o ·ªï ƒëƒ©a ƒë∆∞·ª£c mount l·∫°i"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1728382092138,"user":{"displayName":"VƒÉn Hi·∫øu L√Ω","userId":"02618301898146327018"},"user_tz":-420},"id":"7n077GNnlE7T"},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"bPzh8V9HlE7U"},"source":["# T·∫£i d·ªØ li·ªáu t·ª´ Hugging Face"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1308,"status":"ok","timestamp":1728382093442,"user":{"displayName":"VƒÉn Hi·∫øu L√Ω","userId":"02618301898146327018"},"user_tz":-420},"id":"MErr4xyXlE7W"},"outputs":[],"source":["dataset = load_dataset(\"xDAN-datasets/ChatDoctor_HealthCareMagic_112k\")"]},{"cell_type":"markdown","metadata":{"id":"itMzT-wLlE7W"},"source":["# L·∫•y ra m·ªôt ph·∫ßn d·ªØ li·ªáu"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1728382093443,"user":{"displayName":"VƒÉn Hi·∫øu L√Ω","userId":"02618301898146327018"},"user_tz":-420},"id":"qnNObV3AlE7W"},"outputs":[],"source":["dataset = dataset['train'].select(range(5000))"]},{"cell_type":"markdown","metadata":{"id":"3ikeP_TElE7X"},"source":["# Ki·ªÉm tra d·ªØ li·ªáu"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1728382093443,"user":{"displayName":"VƒÉn Hi·∫øu L√Ω","userId":"02618301898146327018"},"user_tz":-420},"id":"pmrSNKYslE7X"},"outputs":[],"source":["# print(dataset['train'][0])"]},{"cell_type":"markdown","metadata":{"id":"wVAzISYqlE7X"},"source":["# T·∫£i tokenizer v√† model t·ª´ GPT-2"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1109,"status":"ok","timestamp":1728382265946,"user":{"displayName":"VƒÉn Hi·∫øu L√Ω","userId":"02618301898146327018"},"user_tz":-420},"id":"60NZ_JS3lE7X","outputId":"3e865aee-6fb3-438c-9cde-593a9a5861e1"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n","\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"markdown","metadata":{"id":"wZVyYwJUlE7Y"},"source":["# Chu·∫©n b·ªã d·ªØ li·ªáu hu·∫•n luy·ªán"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":378,"status":"ok","timestamp":1728382341168,"user":{"displayName":"VƒÉn Hi·∫øu L√Ω","userId":"02618301898146327018"},"user_tz":-420},"id":"1wAMeHxLlE7Y","outputId":"74c09824-6d99-4671-d6a0-b8986f8fe8c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Warning: Empty inputs or outputs after filtering. Returning empty tensors.\n"]}],"source":["conversations = dataset['conversations']\n","\n","def preprocess_data(conversations):\n","    inputs = []\n","    outputs = []\n","\n","    for conversation in conversations:\n","        # Iterate through the list of dictionaries in 'conversation'\n","        for turn in conversation:\n","            # Now access the 'input' and 'output' keys in each dictionary\n","            input_text = turn.get('input')\n","            output_text = turn.get('output')\n","\n","            # Only append if both 'input' and 'output' are present and not empty strings\n","            if input_text and output_text and input_text.strip() and output_text.strip():  # Check if strings are not empty after stripping whitespace\n","                inputs.append(input_text)\n","                outputs.append(output_text)\n","\n","    # Check if inputs or outputs are empty after filtering\n","    if not inputs or not outputs:\n","        print(\"Warning: Empty inputs or outputs after filtering. Returning empty tensors.\")\n","        return {}, {}  # Return empty dictionaries to avoid tokenizer error\n","\n","    # Token h√≥a c√°c c√¢u h·ªèi v√† c√¢u tr·∫£ l·ªùi\n","    tokenized_inputs = tokenizer(inputs, padding=True, truncation=True, return_tensors=\"pt\")\n","    tokenized_outputs = tokenizer(outputs, padding=True, truncation=True, return_tensors=\"pt\")\n","\n","    return tokenized_inputs, tokenized_outputs\n","\n","# Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu h·ªôi tho·∫°i\n","tokenized_inputs, tokenized_outputs = preprocess_data(conversations)\n"]},{"cell_type":"markdown","metadata":{"id":"nksExUr7lE7Y"},"source":["# Thi·∫øt l·∫≠p c√°c tham s·ªë cho qu√° tr√¨nh fine-tuning"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1728382344841,"user":{"displayName":"VƒÉn Hi·∫øu L√Ω","userId":"02618301898146327018"},"user_tz":-420},"id":"ilhAnT6flE7Y","outputId":"4e81c023-4d9f-4d16-a561-e6d768f88ae2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}],"source":["# Thi·∫øt l·∫≠p tham s·ªë hu·∫•n luy·ªán\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    per_device_train_batch_size=2,\n","    num_train_epochs=3,  # S·ªë l∆∞·ª£ng epoch ƒë·ªÉ hu·∫•n luy·ªán\n","    logging_dir=\"./logs\",\n","    logging_steps=200,\n","    save_steps=1000,\n","    save_total_limit=2,\n","    evaluation_strategy=\"steps\",  # ƒê√°nh gi√° theo t·ª´ng b∆∞·ªõc\n","    eval_steps=500,\n","    load_best_model_at_end=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"xuOLglzclE7Z"},"source":["# T·∫°o m·ªôt Trainer ƒë·ªÉ fine-tune m√¥ h√¨nh"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":354,"status":"error","timestamp":1728382481556,"user":{"displayName":"VƒÉn Hi·∫øu L√Ω","userId":"02618301898146327018"},"user_tz":-420},"id":"hArhWp9WlE7Z","outputId":"2566cf4c-14e0-4d43-bfe9-32293ab92c77"},"outputs":[{"ename":"NameError","evalue":"name 'tokenized_datasets' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-cb8bb27b1850>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"]}],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"LIjNGiYUlE7Z"},"source":["# L∆∞u m√¥ h√¨nh ƒë√£ fine-tune"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1728382093983,"user":{"displayName":"VƒÉn Hi·∫øu L√Ω","userId":"02618301898146327018"},"user_tz":-420},"id":"VLSA9ZVjlE7Z"},"outputs":[],"source":["model.save_pretrained(\"./fine_tuned_chatdoctor\")\n","tokenizer.save_pretrained(\"./fine_tuned_chatdoctor\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
